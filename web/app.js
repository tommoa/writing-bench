var h=(z,K=document)=>K.querySelector(z),A=(z,K=document)=>[...K.querySelectorAll(z)];function q(z,K={},...V){let N=document.createElement(z);for(let[U,X]of Object.entries(K)){if(X==null)continue;if(U==="className"&&typeof X==="string")N.className=X;else if(U.startsWith("on")&&typeof X==="function")N.addEventListener(U.slice(2).toLowerCase(),X);else if(U==="innerHTML"&&typeof X==="string")N.innerHTML=X;else if(typeof X==="string")N.setAttribute(U,X)}for(let U of V)if(typeof U==="string")N.appendChild(document.createTextNode(U));else if(U)N.appendChild(U);return N}var m={index:null},g=null;function s(){let z=new URLSearchParams(location.search),K=z.get("run");if(K)return{page:"run",id:K};if(z.get("page")==="runs")return{page:"runs"};if(z.get("page")==="methodology")return{page:"methodology"};return{page:"dashboard"}}async function qq(){let z=await fetch("data/runs.json");if(!z.ok)throw Error("No data found. Run a benchmark and export first.");return z.json()}async function zq(z){let K=await fetch(`data/runs/${z}.json`);if(!K.ok)throw Error(`Run ${z} not found`);return K.json()}function M(z){let K=h("#app");if(K.innerHTML="",typeof z==="string")K.innerHTML=z;else K.appendChild(z)}function n(z){M(`<div id="error">${z}</div>`)}function a(z){return new Date(z).toLocaleDateString("en-US",{year:"numeric",month:"short",day:"numeric",hour:"2-digit",minute:"2-digit"})}function Kq(z){if(z>=100)return`${Math.round(z)}`;if(z>=10)return z.toFixed(1);return z.toFixed(2)}function o(z){let K=z.usage.inputTokens+z.usage.outputTokens;if(z.fromCache)return q("p",{className:"muted small mt-1"},`${K} tokens | [cached]`);let V=z.usage.cacheReadTokens?` (${z.usage.cacheReadTokens} cached)`:"",N=z.cost.totalUncached!=null&&z.cost.totalUncached>z.cost.total+0.00005?` (uncached: $${z.cost.totalUncached.toFixed(4)})`:"";return q("p",{className:"muted small mt-1"},`${K} tokens${V} | $${z.cost.total.toFixed(4)}${N} | ${(z.latencyMs/1000).toFixed(1)}s`)}function Nq(z){let K=z.usage?z.usage.inputTokens+z.usage.outputTokens:0;if(z.fromCache)return q("p",{className:"muted small mt-1"},`${K} tokens | [cached]`);let V=z.cost?z.cost.total:0,N=z.latencyMs?(z.latencyMs/1000).toFixed(1):"?";return q("p",{className:"muted small mt-1"},`${K} tokens | $${V.toFixed(4)} | ${N}s`)}function Qq(z){let K=document.createDocumentFragment();if(z.cumulativeElo.writing.length>0)K.appendChild(q("h2",{},"Writer ELO")),K.appendChild(f(z.cumulativeElo.writing));if(z.cumulativeElo.feedback.length>0)K.appendChild(q("h2",{},"Feedback Provider ELO")),K.appendChild(f(z.cumulativeElo.feedback));if(z.cumulativeElo.byTag&&Object.keys(z.cumulativeElo.byTag).length>0){K.appendChild(q("h2",{},"ELO by Tag"));for(let[V,N]of Object.entries(z.cumulativeElo.byTag)){let U=q("details");U.appendChild(q("summary",{},V)),U.appendChild(q("div",{className:"details-content"},f(N))),K.appendChild(U)}}if(z.eloHistory.length>1)K.appendChild(q("h2",{},"ELO History")),K.appendChild(Uq(z.eloHistory));if(z.runs.length>0)K.appendChild(q("h2",{},"Recent Runs")),K.appendChild(t(z.runs));if(z.runs.length===0&&z.cumulativeElo.writing.length===0)K.appendChild(q("p",{className:"muted mt-2"},"No benchmark data yet. Run a benchmark and export results."));M(K)}function f(z){let K=q("table"),N=[{key:"initial",label:"Write"},{key:"initialJudging",label:"Judge"},{key:"feedback",label:"Feedback"},{key:"revised",label:"Revise"},{key:"revisedJudging",label:"Re-Judge"}].filter((O)=>z.some((J)=>(J.costByStage?.[O.key]??0)>0)),U=z.some((O)=>O.totalCost!=null&&O.totalCost>0),X=[q("th",{className:"rank"},"#"),q("th",{},"Model"),q("th",{className:"sortable"},"ELO"),q("th",{},"Matches")];if(U){for(let O of N)X.push(q("th",{className:"cost"},O.label));X.push(q("th",{className:"cost"},"Total"))}K.appendChild(q("thead",{},q("tr",{},...X)));let H=q("tbody");return z.forEach((O,J)=>{let Y=J===0?"rating top":J===z.length-1?"rating bottom":"rating",Q=[q("td",{className:"rank"},String(J+1)),q("td",{},O.model),q("td",{className:Y},String(O.rating)),q("td",{className:"muted"},String(O.matchCount))];if(U){for(let _ of N){let F=O.costByStage?.[_.key]??0;Q.push(q("td",{className:"cost"},F>0?`$${F.toFixed(4)}`:"-"))}let Z=O.totalCost??0;Q.push(q("td",{className:"cost total"},Z>0?`$${Z.toFixed(4)}`:"-"))}H.appendChild(q("tr",{},...Q))}),K.appendChild(H),K}function Uq(z){let K=q("div"),V=new Set;z.forEach((N)=>Object.keys(N.ratings).forEach((U)=>V.add(U)));for(let N of V){let U=z.map((F)=>F.ratings[N]).filter((F)=>F!=null);if(U.length<2)continue;let X=Math.min(...U),O=Math.max(...U)-X||1,J=120,Y=30,Q=2,Z=U.map((F,T)=>{let L=Q+T/(U.length-1)*(J-2*Q),R=Q+(1-(F-X)/O)*(Y-2*Q);return`${L},${R}`}),_=`<svg viewBox="0 0 ${J} ${Y}"><path d="M${Z.join("L")}"/></svg>`;K.appendChild(q("div",{className:"mb-1"},q("span",{},N+" "),q("span",{className:"sparkline",innerHTML:_}),q("span",{className:"muted small"},` ${U[U.length-1]}`)))}return K}function t(z){let K=q("ul",{className:"run-list"});for(let V of z){let N=q("a",{href:`?run=${V.id}`},a(V.timestamp)),U=q("span",{className:"run-meta"},`${V.models.join(", ")} | ${V.promptCount} prompts | $${(V.totalCostUncached??V.totalCost).toFixed(4)}`);K.appendChild(q("li",{},N,U))}return K}function Vq(z){let K=document.createDocumentFragment();if(K.appendChild(q("h2",{},"All Runs")),z.runs.length>0)K.appendChild(t(z.runs));else K.appendChild(q("p",{className:"muted mt-2"},"No runs yet. Run a benchmark and export results."));M(K)}function Wq(z){let K=document.createDocumentFragment();K.appendChild(q("p",{},q("a",{href:"?"},"< back to leaderboard"))),K.appendChild(q("h2",{},`Run: ${a(z.config.timestamp)}`));let V=z.config.models.map((Y)=>Y.label).join(", ");if(z.config.judges&&z.config.judges.length>0){let Y=z.config.judges.map((Q)=>Q.label).join(", ");K.appendChild(q("p",{className:"muted"},`Writers: ${V} | Judges: ${Y}`))}else K.appendChild(q("p",{className:"muted"},`Models: ${V}`));let N=z.meta.costByModelByStageUncached??{},U=z.meta.speedByModel;if(K.appendChild(q("h2",{},"Initial Writer ELO")),K.appendChild(j(z.elo.initial.ratings,N,U)),K.appendChild(q("h2",{},"Revised Writer ELO")),K.appendChild(j(z.elo.revised.ratings,N,U)),z.elo.revised.feedbackRatings&&z.elo.revised.feedbackRatings.length>0)K.appendChild(q("h2",{},"Feedback Provider ELO")),K.appendChild(j(z.elo.revised.feedbackRatings,N,U));if(z.elo.initial.byTag&&Object.keys(z.elo.initial.byTag).length>0){K.appendChild(q("h2",{},"ELO by Tag"));for(let[Y,Q]of Object.entries(z.elo.initial.byTag)){let Z=q("details");Z.appendChild(q("summary",{},Y));let _=q("div",{className:"details-content"});if(_.appendChild(q("h4",{},"Initial")),_.appendChild(j(Q,N,U)),z.elo.revised.byTag?.[Y])_.appendChild(q("h4",{},"Revised")),_.appendChild(j(z.elo.revised.byTag[Y],N,U));Z.appendChild(_),K.appendChild(Z)}}let X=q("div",{className:"section-header"});X.appendChild(q("h2",{},"Outputs by Prompt"));let H=document.createElement("select");H.className="prompt-filter-select",H.appendChild(new Option("All prompts","all"));let O=[...new Set(z.config.prompts.flatMap((Y)=>Y.tags))].sort();if(O.length>1)for(let Y of O)H.appendChild(new Option(`Tag: ${Y}`,`tag:${Y}`));for(let Y of z.config.prompts)H.appendChild(new Option(Y.name,`id:${Y.id}`));X.appendChild(H),K.appendChild(X);let J=q("div",{id:"prompt-sections"});for(let Y of z.config.prompts){let Q=Xq(z,Y);Q.setAttribute("data-prompt-id",Y.id),Q.setAttribute("data-prompt-tags",Y.tags.join(",")),J.appendChild(Q)}K.appendChild(J),H.addEventListener("change",()=>{let Y=H.value;for(let Q of A("[data-prompt-id]",J))if(Y==="all")Q.style.display="";else if(Y.startsWith("tag:")){let Z=Y.slice(4),_=(Q.getAttribute("data-prompt-tags")??"").split(",");Q.style.display=_.includes(Z)?"":"none"}else if(Y.startsWith("id:")){let Z=Y.slice(3);Q.style.display=Q.getAttribute("data-prompt-id")===Z?"":"none"}}),K.appendChild(q("h2",{},"Judgments")),K.appendChild(Zq(z)),K.appendChild(q("h2",{},"Run Metadata")),K.appendChild(_q(z)),M(K)}function c(z,K){return q("div",{className:"cost-item"},q("div",{className:"label"},z),q("div",{className:"value"},K))}function j(z,K,V){let N=q("table"),U=[{key:"initial",label:"Write"},{key:"initialJudging",label:"Judge"},{key:"feedback",label:"Feedback"},{key:"revised",label:"Revise"},{key:"revisedJudging",label:"Re-Judge"}],X=K??{},H=z.map((_)=>_.model),O=U.filter((_)=>H.some((F)=>(X[F]?.[_.key]??0)>0)),J=O.length>0,Y=V!=null&&Object.keys(V).length>0,Q=[q("th",{className:"rank"},"#"),q("th",{},"Model"),q("th",{},"ELO"),q("th",{},"W/L/T")];if(J){for(let _ of O)Q.push(q("th",{className:"cost"},_.label));Q.push(q("th",{className:"cost"},"Total"))}if(Y)Q.push(q("th",{},"Speed"));N.appendChild(q("thead",{},q("tr",{},...Q)));let Z=q("tbody");return z.forEach((_,F)=>{let T=F===0?"rating top":F===z.length-1?"rating bottom":"rating",L=`${_.wins}/${_.losses}/${_.ties}`,R=[q("td",{className:"rank"},String(F+1)),q("td",{},_.model),q("td",{className:T},String(_.rating)),q("td",{className:"wlt"},L)];if(J){let D=X[_.model]??{};for(let P of O){let w=D[P.key]??0;R.push(q("td",{className:"cost"},w>0?`$${w.toFixed(4)}`:"-"))}let $=0;for(let P of Object.values(D))$+=P;R.push(q("td",{className:"cost total"},$>0?`$${$.toFixed(4)}`:"-"))}if(Y){let D=V[_.model],$=D?`${Kq(D.tokensPerSecond)} tok/s`:"-";R.push(q("td",{className:"speed"},$))}Z.appendChild(q("tr",{},...R))}),N.appendChild(Z),N}function Xq(z,K){let V=q("details");V.appendChild(q("summary",{},`${K.name} (${K.tags.join(", ")})`));let N=q("div",{className:"details-content"});N.appendChild(q("p",{className:"muted small"},K.description));let U=z.samples.filter((Q)=>Q.promptId===K.id&&Q.stage==="initial").sort((Q,Z)=>Q.model.localeCompare(Z.model)||Q.outputIndex-Z.outputIndex);if(U.length===0)return N.appendChild(q("p",{className:"muted"},"No outputs.")),V.appendChild(N),V;let X=U.length>4,H=q("div"),O=U.map((Q)=>{let _=U.filter((F)=>F.model===Q.model).length>1?` #${Q.outputIndex+1}`:"";return`${Q.model}${_}`});function J(Q){A(".tab-content",H).forEach((_)=>_.classList.remove("active"));let Z=`prompt-${K.id}-${Q}`;h(`#${Z}`,H)?.classList.add("active")}let Y;if(X){let Q=document.createElement("select");Q.className="model-select";for(let Z=0;Z<O.length;Z++)Q.appendChild(new Option(O[Z],String(Z)));Q.addEventListener("change",()=>{J(Number(Q.value))}),Y=q("div",{className:"tabs tabs-dropdown"}),Y.appendChild(Q)}else{Y=q("div",{className:"tabs"});for(let Q=0;Q<O.length;Q++){let Z=q("button",{className:Q===0?"tab active":"tab",onClick:()=>{A(".tab",Y).forEach((_)=>_.classList.remove("active")),Z.classList.add("active"),J(Q)}},O[Q]);Y.appendChild(Z)}}return U.forEach((Q,Z)=>{let _=`prompt-${K.id}-${Z}`,F=q("div",{id:_,"data-sample-id":Q.id,className:Z===0?"tab-content active":"tab-content"});F.appendChild(q("div",{className:"output-text"},Q.text)),F.appendChild(o(Q));let T=z.judgments.filter((R)=>R.sampleA===Q.id||R.sampleB===Q.id).length;if(T>0)F.appendChild(q("button",{className:"view-judgments-btn",onClick:()=>g?.focusSample(Q.id)},`view ${T} judgments →`));let L=z.feedback.filter((R)=>R.targetSampleId===Q.id).sort((R,D)=>R.sourceModel.localeCompare(D.sourceModel));for(let R of L){let D=q("details");D.appendChild(q("summary",{},`Feedback from ${R.sourceModel}`));let $=q("div",{className:"details-content"}),P=q("div",{className:"feedback-text"});P.appendChild(document.createTextNode(R.text)),P.appendChild(Nq(R)),$.appendChild(P);let w=z.samples.find((k)=>k.stage==="revised"&&k.feedbackUsed===R.id&&k.promptId===K.id);if(w){let k=q("div",{className:"revision-nested",id:`sample-${w.id}`});k.appendChild(q("div",{className:"muted small"},`${w.model}'s revision:`)),k.appendChild(q("div",{className:"output-text"},w.text)),k.appendChild(o(w));let C=z.judgments.filter((x)=>x.sampleA===w.id||x.sampleB===w.id).length;if(C>0)k.appendChild(q("button",{className:"view-judgments-btn",onClick:()=>g?.focusSample(w.id)},`view ${C} judgments →`));$.appendChild(k)}D.appendChild($),F.appendChild(D)}H.appendChild(F)}),N.appendChild(Y),N.appendChild(H),V.appendChild(N),V}function p(z){let K=z.parentElement;while(K){if(K.tagName==="DETAILS")K.setAttribute("open","");K=K.parentElement}}function d(z,K){let V=K.samples.find((X)=>X.id===z);if(!V)return;let N=K.samples.filter((X)=>X.promptId===V.promptId&&X.stage==="initial").sort((X,H)=>X.model.localeCompare(H.model)||X.outputIndex-H.outputIndex),U=null;if(V.stage==="initial"){let X=N.findIndex((J)=>J.id===z);if(X===-1)return;let H=`prompt-${V.promptId}-${X}`,O=h(`#${H}`);if(!O)return;p(O),i(O,X),U=O}else{let X=h(`#sample-${z}`);if(!X)return;if(p(X),V.originalSampleId){let H=N.findIndex((O)=>O.id===V.originalSampleId);if(H!==-1){let O=`prompt-${V.promptId}-${H}`,J=h(`#${O}`);if(J)i(J,H)}}U=X}if(U)U.scrollIntoView({behavior:"smooth",block:"start"}),U.classList.add("scroll-highlight"),U.addEventListener("animationend",()=>U.classList.remove("scroll-highlight"),{once:!0})}function i(z,K){let V=z.parentElement;if(!V)return;let N=V.previousElementSibling;if(N&&N.classList.contains("tabs")){let U=h("select.model-select",N);if(U)U.value=String(K);A(".tab",N).forEach((X,H)=>{X.classList.toggle("active",H===K)})}A(".tab-content",V).forEach((U)=>U.classList.remove("active")),z.classList.add("active")}function Yq(z,K){let V=K.get(z.sampleA),N=K.get(z.sampleB);if(z.stage==="improvement"){let U=V?.stage==="initial",X=U?V:N,H=U?N:V,O=H?.feedbackModel??"?",J=`${X?.model??"?"} (original)`,Y=`${H?.model??"?"} (revised, fb: ${O})`,Q;if(z.winner==="tie")Q="Tie";else Q=(z.winner==="A"?V:N)?.stage==="initial"?"original":"revised";return U?{labelA:J,labelB:Y,winnerLabel:Q}:{labelA:Y,labelB:J,winnerLabel:Q}}if(z.stage==="revised"){let U=V?.feedbackModel?` (fb: ${V.feedbackModel})`:"",X=N?.feedbackModel?` (fb: ${N.feedbackModel})`:"";return{labelA:`${V?.model??"?"}${U}`,labelB:`${N?.model??"?"}${X}`,winnerLabel:z.winner==="tie"?"Tie":z.winner==="A"?V?.model??"?":N?.model??"?"}}return{labelA:V?.model??"?",labelB:N?.model??"?",winnerLabel:z.winner==="tie"?"Tie":z.winner==="A"?V?.model??"?":N?.model??"?"}}function Zq(z){let K=q("div");K.id="judgments-section";let V=z.judgments;if(V.length===0)return K.appendChild(q("p",{className:"muted"},"No judgments.")),K;let N=new Map(z.samples.map((W)=>[W.id,W])),U=[...new Set(V.map((W)=>W.stage))].sort(),X=[...new Set(V.map((W)=>W.judgeModel))].sort(),H=[...new Set(z.samples.map((W)=>W.model))].sort(),O=z.config.prompts,J=[...new Set(O.flatMap((W)=>W.tags))].sort(),Y="all",Q="all",Z="all",_="all",F="all",T=null,L=q("div",{className:"judgment-filters"}),R=document.createElement("select");if(R.appendChild(new Option("All prompts","all")),J.length>1)for(let W of J)R.appendChild(new Option(`Tag: ${W}`,`tag:${W}`));for(let W of O)R.appendChild(new Option(W.name,`id:${W.id}`));let D=document.createElement("select");D.appendChild(new Option("All stages","all"));for(let W of U)D.appendChild(new Option(W,W));let $=document.createElement("select");$.appendChild(new Option("All judges","all"));for(let W of X)$.appendChild(new Option(W,W));let P=document.createElement("select");P.appendChild(new Option("All models","all"));for(let W of H)P.appendChild(new Option(W,W));let w=document.createElement("select");w.appendChild(new Option("All models","all"));for(let W of H)w.appendChild(new Option(W,W));L.appendChild(q("span",{className:"muted small"},"Prompt: ")),L.appendChild(R),L.appendChild(q("span",{className:"muted small"}," Stage: ")),L.appendChild(D),L.appendChild(q("span",{className:"muted small"}," Judge: ")),L.appendChild($),L.appendChild(q("span",{className:"muted small"}," Model A: ")),L.appendChild(P),L.appendChild(q("span",{className:"muted small"}," vs B: ")),L.appendChild(w),K.appendChild(L);let k=q("div",{className:"sample-filter-badge"});k.style.display="none",K.appendChild(k);let C=q("div",{className:"h2h-summary"});C.style.display="none",K.appendChild(C);let x=q("div");K.appendChild(x);let I=()=>{let W=V;if(T){W=W.filter((E)=>E.sampleA===T||E.sampleB===T);let G=N.get(T);k.innerHTML="",k.style.display="flex",k.appendChild(q("span",{},`Showing judgments for ${G?.model??"unknown"}'s output`)),k.appendChild(q("button",{className:"badge-clear",onClick:()=>{T=null,I()}},"✕ clear"))}else k.style.display="none";if(F!=="all"){if(F.startsWith("tag:")){let G=F.slice(4),E=new Set(O.filter((y)=>y.tags.includes(G)).map((y)=>y.id));W=W.filter((y)=>E.has(y.promptId))}else if(F.startsWith("id:")){let G=F.slice(3);W=W.filter((E)=>E.promptId===G)}}if(Y!=="all")W=W.filter((G)=>G.stage===Y);if(Q!=="all")W=W.filter((G)=>G.judgeModel===Q);if(Z!=="all"&&_!=="all")W=W.filter((G)=>{let E=N.get(G.sampleA)?.model,y=N.get(G.sampleB)?.model;return E===Z&&y===_||E===_&&y===Z});else if(Z!=="all")W=W.filter((G)=>{let E=N.get(G.sampleA)?.model,y=N.get(G.sampleB)?.model;return E===Z||y===Z});else if(_!=="all")W=W.filter((G)=>{let E=N.get(G.sampleA)?.model,y=N.get(G.sampleB)?.model;return E===_||y===_});if(Z!=="all"&&_!=="all"&&Z!==_){let G=0,E=0,y=0;for(let v of W){let b=N.get(v.sampleA)?.model;if(v.winner==="tie")y++;else if(v.winner==="A")if(b===Z)G++;else E++;else if(b===Z)E++;else G++}C.innerHTML="",C.style.display="block",C.appendChild(q("div",{className:"h2h-record"},q("span",{className:"h2h-model"},Z),q("span",{className:"h2h-wins"},` ${G}W`),q("span",{className:"muted"}," / "),q("span",{className:"h2h-losses"},`${E}L`),q("span",{className:"muted"}," / "),q("span",{className:"h2h-ties"},`${y}T`),q("span",{className:"muted"}," vs "),q("span",{className:"h2h-model"},_)))}else C.style.display="none";x.innerHTML="",x.appendChild(q("p",{className:"muted small mb-1"},`${W.length} of ${V.length} judgments`));for(let G of W){let E=z.config.prompts.find((S)=>S.id===G.promptId),{labelA:y,labelB:v,winnerLabel:b}=Yq(G,N),r=G.winner==="A"?"a":G.winner==="B"?"b":"tie",l=q("a",{href:"#",className:"judgment-sample-link",onClick:(S)=>{S.preventDefault(),d(G.sampleA,z)}},y),e=q("a",{href:"#",className:"judgment-sample-link",onClick:(S)=>{S.preventDefault(),d(G.sampleB,z)}},v),B=q("div",{className:"judgment"});if(B.appendChild(q("div",{className:"judgment-header"},q("span",{className:"judgment-stage"},G.stage),q("span",{},` ${E?.name??G.promptId}`),q("span",{className:"judgment-judge"},`Judge: ${G.judgeModel}`))),B.appendChild(q("div",{className:"judgment-matchup"},l,q("span",{className:"muted"}," vs "),e)),B.appendChild(q("div",{className:"judgment-result"},q("span",{className:"muted"},"→ "),q("span",{className:`judgment-winner ${r}`},b))),G.reasoning)B.appendChild(q("div",{className:"judgment-reasoning"},G.reasoning));x.appendChild(B)}};return R.addEventListener("change",()=>{F=R.value,T=null,I()}),D.addEventListener("change",()=>{Y=D.value,T=null,I()}),$.addEventListener("change",()=>{Q=$.value,T=null,I()}),P.addEventListener("change",()=>{Z=P.value,T=null,I()}),w.addEventListener("change",()=>{_=w.value,T=null,I()}),g={focusSample(W){let G=N.get(W);T=W,F="all",Y="all",Q="all",Z=G?.model??"all",_="all",R.value="all",D.value="all",$.value="all",P.value=Z,w.value="all",I(),K.scrollIntoView({behavior:"smooth"})},focusModel(W){T=null,F="all",Z=W,_="all",Y="all",Q="all",R.value="all",D.value="all",$.value="all",P.value=W,w.value="all",I(),K.scrollIntoView({behavior:"smooth"})}},I(),K}function _q(z){let K=q("div"),V=q("div",{className:"cost-grid"}),N=z.meta.totalCostUncached??z.meta.totalCost;if(V.appendChild(c("Total Cost",`$${N.toFixed(4)}`)),V.appendChild(c("Duration",`${(z.meta.durationMs/1000).toFixed(1)}s`)),V.appendChild(c("Total Tokens",z.meta.totalTokens.toLocaleString())),K.appendChild(V),z.modelInfo&&Object.keys(z.modelInfo).length>0){K.appendChild(q("h3",{},"Models"));let U=q("div",{className:"model-cards"});for(let[X,H]of Object.entries(z.modelInfo))U.appendChild(q("div",{className:"model-card"},q("div",{className:"name"},X),q("div",{className:"detail"},H.name),q("div",{className:"detail"},`Family: ${H.family}`),q("div",{className:"detail"},`$${H.costPer1MInput}/M in, $${H.costPer1MOutput}/M out`),H.releaseDate?q("div",{className:"detail"},`Released: ${H.releaseDate}`):null,q("div",{className:"detail"},H.openWeights?"Open weights":"Proprietary")));K.appendChild(U)}return K}function Hq(){let z=q("div",{className:"methodology"},q("h2",{},"How Models Are Compared"),q("p",{},'Writing quality is evaluated through pairwise blind judging. For each prompt, an LLM judge is shown two writing samples labeled "Sample A" and "Sample B" with no indication of which model produced which text. The judge decides which sample is better (A, B, or tie) and provides reasoning.'),q("p",{},"Each prompt defines its own judging criteria tailored to the genre. A sermon prompt might specify theological accuracy and pastoral warmth, while a short story prompt might focus on narrative voice and character interiority. The judge evaluates against all listed criteria holistically."),q("p",{},"Judging uses structured JSON output (a Zod schema requesting winner and reasoning). If a judge model does not support structured output, the system falls back to free-text generation and extracts JSON from the response."),q("h3",{},"Position Bias Mitigation"),q("p",{},"LLMs can exhibit position bias — a tendency to favor whichever sample "+"appears first. To counteract this, the benchmark randomly swaps the presentation order of each pair with 50% probability. After the judge responds, the winner is mapped back to the canonical ordering. This ensures that any position preference cancels out over many comparisons."),q("h2",{},"The Benchmark Pipeline"),q("p",{},"The benchmark runs as a reactive pipeline. Tasks fire as soon as their dependencies are met rather than waiting for entire stages to complete. Judging begins as soon as two samples for the same prompt exist; feedback starts as soon as a sample is written; revisions start as soon as feedback arrives."),q("ol",{},q("li",{},q("strong",{},"Write")," — Each model generates an output for each prompt."),q("li",{},q("strong",{},"Judge (initial)")," — Pairwise blind comparison of initial outputs. Every unique "+"pair of samples for a prompt is judged by every judge model."),q("li",{},q("strong",{},"Feedback")," — Each model critiques every other model’s initial output, "+"identifying strengths and areas for improvement."),q("li",{},q("strong",{},"Revise")," — The original writer revises its piece using another model’s "+"feedback."),q("li",{},q("strong",{},"Judge (revised)")," — Revised outputs are compared head-to-head. Only revisions "+"that used feedback from the same source model are compared, so the comparison isolates writing ability from feedback quality."),q("li",{},q("strong",{},"Judge (improvement)")," — Each revision is compared against its own original to "+"measure whether the feedback actually helped improve the writing.")),q("h2",{},"Bradley-Terry Rating System"),q("p",{},"Ratings are computed using the Bradley-Terry model, a maximum likelihood estimation method for pairwise comparison data. Unlike sequential ELO (where processing the same judgments in a different order gives different ratings), Bradley-Terry computes strength parameters from all outcomes simultaneously. The same set of judgments always produces the same ratings."),q("h3",{},"The Algorithm"),q("p",{},"Each model is assigned a strength parameter p, initially set to 1. The algorithm iterates:"),q("div",{className:"formula"},`For each model i:
`+`  scoreᵢ = winsᵢ + 0.5 × tiesᵢ
`+`  expectedᵢ = Σⱼ Nᵢⱼ × pᵢ / (pᵢ + pⱼ)
`+`  pᵢ ← (scoreᵢ / expectedᵢ) × pᵢ

`+`Normalize all strengths by their geometric mean.
`+"Repeat until convergence (max relative change < 10⁻⁶, up to 50 iterations)."),q("p",{},"A model’s strength increases when its observed win "+"rate exceeds what the current strength estimates predict, and decreases when it falls short. Ties count as half a win for each side. The geometric mean normalization prevents strengths from drifting to infinity."),q("h3",{},"ELO-Scale Conversion"),q("p",{},"Bradley-Terry strengths are converted to a familiar ELO-like scale:"),q("div",{className:"formula"},"rating = 400 × log₁₀(strength) + 1500"),q("p",{},"This means a model whose BT strength is 10× another’s will be "+"rated 400 points higher, matching the standard ELO interpretation where a 400-point gap implies roughly 10:1 win odds."),q("h2",{},"Three Rating Types"),q("h3",{},"Writing ELO"),q("p",{},"Direct head-to-head writing quality. Two writing samples for the same prompt are shown to a judge; the winning model gets credit. Both initial and revised stage judgments contribute to writing ratings."),q("h3",{},"Feedback ELO"),q("p",{},"How useful a model’s editorial feedback is, measured indirectly. "+"The system does not compare feedback texts directly. Instead, it uses improvement judgments (revision vs. original) to determine whether feedback led to a better revision."),q("p",{},"The algorithm groups improvement judgments by prompt and judge, then pairs up different feedback providers within each group. If "+"feedback model A’s revision beat the original but feedback model "+"B’s did not, A wins. If both improved or both failed, it’s a tie. "+"These synthetic pairwise outcomes are then fed into the same Bradley-Terry computation."),q("h3",{},"Per-Tag ELO"),q("p",{},'Each prompt has genre tags (e.g. "speech", "theological", "creative"). Per-tag ratings run the same Bradley-Terry computation restricted to judgments from prompts with a given tag. This reveals '+"category-specific strengths — a model might excel at essays but "+"struggle with creative fiction."),q("h2",{},"Cumulative Ratings"),q("p",{},"Ratings accumulate across multiple benchmark runs. Rather than applying sequential updates (which would be order-dependent), the system stores pairwise records: for each pair of models, the total number of wins for each side and ties."),q("p",{},"When a new run completes, its pairwise outcomes are merged with the existing accumulated records. Ratings are then recomputed from scratch using Bradley-Terry on the full merged dataset. This means the order in which runs are processed does not affect the final ratings."),q("p",{},"The leaderboard on the dashboard page always reflects the cumulative ratings across all runs. Individual run pages show "+"ratings computed from that run’s judgments alone."),q("h2",{},"Reading the Results"),q("ul",{},q("li",{},q("strong",{},"1500")," is the baseline rating. A model with no wins or losses, or one at the geometric mean of all model strengths, sits at 1500."),q("li",{},q("strong",{},"400-point gap")," corresponds to roughly 10:1 expected win odds. A model rated 1900 is expected to beat a 1500-rated model about 90% of the time."),q("li",{},q("strong",{},"W / L / T")," are raw win, loss, and tie counts from all pairwise matches the model participated in. These are the direct inputs to the Bradley-Terry computation."),q("li",{},q("strong",{},"Matches")," is the total number of pairwise comparisons involving the model (W + L + T). More matches produce more reliable ratings.")),q("p",{className:"note"},"Ratings from a small number of matches should be interpreted cautiously. As more runs accumulate, the cumulative ratings converge toward stable values."));M(z.outerHTML)}async function Oq(){A(".nav a[data-page]").forEach((z)=>{z.addEventListener("click",(K)=>{K.preventDefault(),history.pushState(null,"",z.getAttribute("href")),u()})}),window.addEventListener("popstate",u);try{m.index=await qq()}catch(z){if(s().page!=="methodology"){n(z instanceof Error?z.message:String(z));return}}u()}function u(){let{page:z,id:K}=s();switch(g=null,A(".nav a").forEach((V)=>{let N=V.getAttribute("data-page"),U=N===z||z==="run"&&N==="runs";V.classList.toggle("active",U)}),z){case"dashboard":Qq(m.index);break;case"runs":Vq(m.index);break;case"run":Gq(K);break;case"methodology":Hq();break}}async function Gq(z){M('<div id="loading">loading run...</div>');try{let K=await zq(z);Wq(K)}catch(K){n(K instanceof Error?K.message:String(K))}}Oq();
